{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"300\" src=\"https://data-services.hosting.nyu.edu/assets/libraries_short_color.png\" alt=\"NYU Libraries Logo\">\n",
    "\n",
    "# Deidentification by Design: Creating Ethical Data Derivatives with Python\n",
    "\n",
    "**Overview** \n",
    "\n",
    "Research and proprietary data often contain personally identifiable information, with variables that reveal details about the lives of individuals and may have been collected without the personâ€™s knowledge or consent. Datasets aggregated at the individual level often interest social science scholars, yet such data poses a risk of identification and create an ethical dilemma for curators.\n",
    " \n",
    "While some types of information and data are legally protected, other social data, such as home mortgage files, voter registration files, and tax parcel records are public and are often augmented with modeled indicators, such as religious belief or personal income, that may not represent the reality of people's lives.  Library information and data specialists must develop infrastructure, workflows, and policies to ensure the ethical stewardship and use of these datasets.  This interactive workshop will explore the tension between making purchased data as widely accessible to researchers as possible, while also ensuring that sensitive data is not abused. \n",
    " \n",
    "Following a short discussion of some of the above challenges, we will introduce technologies and workflows for data de-identification via hands-on activities in you will create redacted samples of data that maintain research integrity and usefulness. Learning outcomes include:\n",
    " \n",
    "1) Develop fluency with generating random samples in order to make analysis with large files more manageable\n",
    "2) Know how to assess the identification risk of specific variables within a dataset in order to protect the identity of human subjects\n",
    "3) Create a Jupyter Notebook workflow that enables cleaning, redacting, and sharing data for research use\n",
    "4) Learn some fundamental Pandas features for exploring, cleaning, and transforming data\n",
    "\n",
    "**About the Sample Data:**\n",
    "\n",
    "The L2 Political files come in two parts for each state: Demographic indicators and Voter History. Both are .tab files (tab delimited) and will need to be extracted. It is likely that you'll need to use a command line tool like The Unarchiver to decompress each archive. \n",
    "\n",
    "For larger states, you will want to redact parts of these files by only loading the minimum number of variables that you need into memory, merge them together, then sample them in order to perform analysis.\n",
    "\n",
    "This notebook is designed to provide some guidance on working with the data and creating extracts in a memory-efficient manner. It uses Python Libraries that are included with the Anacondas distribution, with the exception of [Pyjanitor](https://pyjanitor.readthedocs.io/index.html). In order to install it, run\n",
    "\n",
    "`conda install pyjanitor -c conda-forge`\n",
    "\n",
    "The conda distribution here seems to work the best (as opposed to pip). See the documentation for each of these libraries (especially Pandas) for more information. You will need to restart your Kernel and notebook before proceeding if you haven't already installed Pyjanitor. \n",
    "\n",
    "Users are expected to have agreed to the Terms of Use and should create redacted versions of the data that strip out identifying information and load only variables that are important for analysis. For the latest documentation, visit the [L2 Political LibGuide](https://guides.nyu.edu/l2political)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Loading Python modules and L2 Data\n",
    "\n",
    "Begin by importing some basic libraries to work with the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'janitor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1563e7f9967e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjanitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'janitor'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import janitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading .tab files**\n",
    "\n",
    "First, we read in the .tab file of demographic variables, declare the tab as the separator, and set the encoding\n",
    "\n",
    "There are 646 columns in the demographic file, and it's better to set the datetype as string (labeled \"object\" in Pandas) for each of them and only load certian columns to cut down on memory usage.\n",
    "\n",
    "Declare the columns you want; see the codebook for the full list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'VM2--WY--2021-01-13-DEMOGRAPHIC.tab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cab02865675d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m## to prevent Pandas from guessing the encoding or data types incorrectly (and wasting resources)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m state_demographic = pd.read_csv('VM2--WY--2021-01-13-DEMOGRAPHIC.tab', \n\u001b[0m\u001b[1;32m     14\u001b[0m                                 \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unicode_escape'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                 usecols=selected_variables)\n",
      "\u001b[0;32m~/desktop/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/desktop/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/desktop/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/desktop/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/desktop/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1989\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1991\u001b[0;31m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1992\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'VM2--WY--2021-01-13-DEMOGRAPHIC.tab'"
     ]
    }
   ],
   "source": [
    "selected_variables = ['LALVOTERID',\n",
    "                      'Residence_Addresses_Latitude',\n",
    "                      'Residence_Addresses_Longitude',\n",
    "                      'Voters_Gender',\n",
    "                      'Voters_FIPS']\n",
    "\n",
    "## Change the file path below accordingly to point toward full path the relevant file. \n",
    "## Note that this data loading step below will take the longest out of any step for your computer to process\n",
    "## The arguments that set the encoding, define the sep, load only some variables,\n",
    "## and force all columns into a string are necessary\n",
    "## to prevent Pandas from guessing the encoding or data types incorrectly (and wasting resources)\n",
    "\n",
    "state_demographic = pd.read_csv('VM2--WY--2021-01-13-DEMOGRAPHIC.tab', \n",
    "                                sep='\\t', dtype=str, encoding='unicode_escape',\n",
    "                                usecols=selected_variables)\n",
    "\n",
    "## Preview of the first 5 rows of the data subset. \n",
    "\n",
    "state_demographic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also not a bad idea to test the length of the entire file and verify it with the number of records on the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(state_demographic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run .info to get a breakdown, including how much memory the data frame is eating up.\n",
    "\n",
    "This will give you context for how much your computer can handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_demographic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will read into memory the voter history file that corresponds to the state.\n",
    "\n",
    "This time, we make sure to include the *LALVOTERID* column, and we will add whether or not they voted in 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_variables = ['LALVOTERID', 'General_2016_11_08']\n",
    "\n",
    "state_voterhistory = pd.read_csv('VM2--WY--2021-01-13-VOTEHISTORY.tab',\n",
    "                                 sep='\\t', dtype=str, encoding='unicode_escape',\n",
    "                                 usecols=needed_variables)\n",
    "                                \n",
    "state_voterhistory.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Again, I will want to run an .info command to test the size and memory\n",
    "\n",
    "state_voterhistory.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Join of Demographic and Voter History Tables\n",
    "\n",
    "Now that I have established a two reduced tables with a connection, I can join them (using the merge method in Pandas) based on the *LALVOTERID* common key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_file = pd.merge(state_voterhistory, state_demographic,\n",
    "                       how='left', left_on='LALVOTERID', right_on='LALVOTERID')\n",
    "\n",
    "merged_file.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Sampling the Records\n",
    "\n",
    "From here, you can sample the records. The frac argument is in percentage, so here, it's a one percent sample note that the sample function also guesses the dtype (which is proccess intensive) so it's better to do the sample after the final merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_sample = merged_file.sample(frac=0.01)\n",
    "\n",
    "state_sample.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, print out the info to see that its a valid sample size (actually one percent) and that it is using much less memory\n",
    "\n",
    "You'll know that the sampling worked if the index numbers are nonsequential and random also, you can test the length and info of your new sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Further Deidentifying Geocoded Locations: Jittering\n",
    "\n",
    "There are many other filtering and cleaning steps that could be taken before writing out a new file.\n",
    "\n",
    "For example, the coordinates can be identifying, even if you strip out all of the other PID variables, because they are geocoded to the address. You will want to jitter them if you intend to aggregate or visualize data.\n",
    "\n",
    "First, print out the data types to make sure what they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sample.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In order to jitter the coordinates, you need to change them to a float\n",
    "\n",
    "state_sample['Residence_Addresses_Latitude'] = pd.to_numeric(state_sample['Residence_Addresses_Latitude']) \n",
    "state_sample['Residence_Addresses_Longitude'] = pd.to_numeric(state_sample['Residence_Addresses_Longitude']) \n",
    "\n",
    "state_sample.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module pyjanitor introduces a jitter method for dataframes that can be used to displace the exact coordinates of a location.\n",
    "\n",
    "We'll start with jittering the latitude. This library inserts a Gaussian distribution (\"noise\") equal to the standard deviation of the scale. The exact amount of the scale in jitter may need to change but this is a good starting place. You may want to test the results in a mapping software like QGIS or ArcMap before proceeding too far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jittered_lat = state_sample.jitter(\n",
    "    column_name='Residence_Addresses_Latitude',\n",
    "    dest_column_name='lat_jitter',\n",
    "    scale=0.05,\n",
    "    clip=None,\n",
    "    random_state=None,\n",
    ")\n",
    "\n",
    "jittered_lat.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Now we jitter the longitude\n",
    "\n",
    "jittered_long = state_sample.jitter(\n",
    "    column_name='Residence_Addresses_Longitude',\n",
    "    dest_column_name='long_jitter',\n",
    "    scale=0.05,\n",
    "    clip=None,\n",
    "    random_state=None,\n",
    ")\n",
    "\n",
    "jittered_long.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the jittering process, make sure you drop off the actual Residence_Addresses columns.\n",
    "\n",
    "With this last dataframe, you should have a one percent sample that has jittered lat long coordinates, voter gender, whether or not they voted in 2016, and their unique ID:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_sample = jittered_long.drop(columns=['Residence_Addresses_Latitude', 'Residence_Addresses_Longitude'])\n",
    "\n",
    "stripped_sample.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Saving the Resulting Dataframe to a CSV\n",
    "\n",
    "Finally, you may want to write out the sample to a CSV, leaving out the index that Pandas creates and stipulating the encoding into utf-8. Remember to set the full path on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_sample.to_csv(r'wyoming-onepercent-sample.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the course of your analysis, you may want to repeat this process for multiple states and/or develop an aggregate roster of voters culled from a subset. You are encouraged to visit the tools section on the [L2 Political LibGuide at NYU Libraries](https://guides.nyu.edu/l2political/tools).\n",
    "\n",
    "In general, you should have a data management plan with these files that includes the following elements:\n",
    "\n",
    "- Secure any original .tab files you've downloaded by storing them in a single directory location, and then deleting the original .tab files once you've generated extracts\n",
    "- Manage your extracted samples in a secure environment\n",
    "\n",
    "Simple scripts and suggestions are available on the L2 Political LibGuide. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
